model_name: skipgram

dataset: WikiText2
data_dir: data/
train_batch_size: 512
val_batch_size: 512

optimizer: Adam
learning_rate: 0.1
epochs: 10
train_steps:

checkpoint_frequency: 
model_dir: word2box_pytorch/weights/skipgram_WikiText2

skipgram_n_words: 20
min_word_frequency: 50
max_sequence_length: 256
neg_count: 1
embed_dimension: 8
embed_max_norm: 1
n_print: 25

