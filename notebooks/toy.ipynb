{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fmollica/Desktop/word2box_pytorch/notebooks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 1,\n",
       " 'model_state_dict': OrderedDict([('embeddings_word.weight',\n",
       "               tensor([[-1.5760,  2.2762, -1.7630, -2.8536, -1.7968, -2.0962],\n",
       "                       [-1.2729,  0.1896,  0.5583, -3.2420, -2.1034, -3.5221],\n",
       "                       [-1.5969, -0.0850, -1.3632, -4.7778, -1.4891, -1.7271],\n",
       "                       ...,\n",
       "                       [-1.2795,  0.3822,  0.7129, -1.8887, -0.5225, -3.3043],\n",
       "                       [ 0.0722,  0.4643,  0.2189, -3.3266, -1.1419, -1.3767],\n",
       "                       [-1.2206,  0.5306,  0.3791, -4.4984, -0.8685, -1.4596]],\n",
       "                      device='cuda:0')),\n",
       "              ('embeddings_context.weight',\n",
       "               tensor([[  9.8775,  -8.4073,   7.9298,   0.0169,  -4.7633,   0.1899],\n",
       "                       [  9.3574,  -9.5529, -10.1228,   0.1973,  -4.8203,  -5.3279],\n",
       "                       [ 10.3608,  -9.9401,   9.2183,  -0.5697,  -5.6455,  -0.1321],\n",
       "                       ...,\n",
       "                       [  8.8847,   8.7381, -10.1818,  -1.1591,  -1.1446,  -6.9648],\n",
       "                       [  9.6719,   8.8532,   8.5249,  -1.8520,  -2.5287,  -2.2924],\n",
       "                       [  9.2033,   9.0256,   8.3944,  -2.8595,  -1.0289,  -2.1593]],\n",
       "                      device='cuda:0'))]),\n",
       " 'optimizer_state_dict': {'state': {0: {'step': 50411,\n",
       "    'exp_avg': tensor([[-1.4153e-10,  3.0805e-16, -3.9891e-06,  8.7749e-33,  4.3817e-17,\n",
       "              3.9301e-30],\n",
       "            [-4.2840e-13, -9.5920e-21, -2.5307e-22,  4.3816e-34,  2.5094e-27,\n",
       "             -7.2575e-24],\n",
       "            [-1.9686e-20,  7.8463e-27,  4.9185e-29,  5.6052e-45,  3.2053e-28,\n",
       "              5.6944e-31],\n",
       "            ...,\n",
       "            [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45],\n",
       "            [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45],\n",
       "            [ 2.8026e-44,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[2.0468e-21, 3.9253e-23, 1.5913e-12, 6.7750e-25, 5.4541e-26, 2.3787e-24],\n",
       "            [6.9838e-22, 1.2529e-22, 7.2495e-23, 1.0620e-14, 1.2144e-20, 3.9124e-23],\n",
       "            [3.1761e-11, 1.6713e-13, 3.2225e-16, 3.6972e-12, 2.9702e-18, 2.2547e-22],\n",
       "            ...,\n",
       "            [9.4589e-07, 2.4591e-22, 1.9760e-29, 5.9476e-08, 3.4098e-23, 2.4797e-32],\n",
       "            [4.7283e-06, 2.1404e-23, 2.7434e-30, 1.4202e-08, 1.2537e-24, 1.1146e-31],\n",
       "            [7.4795e-12, 1.6833e-26, 2.0430e-29, 5.8741e-15, 1.4328e-27, 7.2616e-31]],\n",
       "           device='cuda:0')},\n",
       "   1: {'step': 50411,\n",
       "    'exp_avg': tensor([[-5.8716e-36,  2.7539e-25,  1.8170e-19,  5.6052e-45,  2.3311e-27,\n",
       "              5.6052e-45],\n",
       "            [-1.0338e-30,  6.6339e-25,  3.3182e-25,  5.6052e-45,  5.3069e-27,\n",
       "              1.6029e-27],\n",
       "            [-5.6375e-36,  4.6472e-31, -7.3095e-26,  5.6052e-45,  1.6362e-33,\n",
       "              5.6052e-45],\n",
       "            ...,\n",
       "            [-3.6443e-29, -8.7583e-24,  1.3450e-23,  5.6052e-45,  5.6052e-45,\n",
       "              1.2692e-26],\n",
       "            [-2.3498e-40, -1.4570e-27, -1.2794e-23,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45],\n",
       "            [-9.9178e-31, -6.9896e-28, -1.1959e-18,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[2.9946e-13, 2.9824e-15, 1.6657e-15, 4.6620e-26, 1.3540e-18, 5.8732e-26],\n",
       "            [1.2119e-12, 1.4333e-17, 1.6676e-16, 3.2534e-26, 1.0092e-21, 1.2373e-20],\n",
       "            [1.3263e-16, 2.1266e-17, 1.8174e-19, 1.4306e-26, 2.7315e-22, 1.1403e-26],\n",
       "            ...,\n",
       "            [1.5074e-15, 1.2014e-21, 1.8483e-14, 8.6141e-29, 3.4020e-29, 6.3836e-20],\n",
       "            [2.3978e-12, 1.4878e-15, 9.8672e-15, 3.3617e-29, 8.5412e-29, 1.1321e-29],\n",
       "            [4.7161e-13, 4.0050e-19, 2.0938e-12, 8.1555e-30, 1.2289e-28, 7.6578e-29]],\n",
       "           device='cuda:0')}},\n",
       "  'param_groups': [{'lr': 0.0006250000000000001,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'eps': 1e-08,\n",
       "    'weight_decay': 0,\n",
       "    'amsgrad': False,\n",
       "    'initial_lr': 0.025,\n",
       "    'params': [0, 1]}]},\n",
       " 'loss': tensor(-359.2028, device='cuda:0', requires_grad=True)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "print(os.getcwd())\n",
    "model = torch.load(\"/home/fmollica/Desktop/word2box_pytorch/weights/skipgram_WikiText2/epochs_5_min_count_50_batch_size_128_embed_dim_3_lr_0.025_window_5_neg_count_5/checkpoint_epoch_1.pt\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_2(word, N=None):\n",
    "\n",
    "        if N is None:\n",
    "            N = len(model.vocab)\n",
    "\n",
    "        embedding_all_target = model.embeddings_word.all_boxes\n",
    "\n",
    "        \n",
    "        index_word = (model.vocab[word])\n",
    "        embedding_word = embedding_all_target[index_word]\n",
    "                \n",
    "        # sim3 = torch.exp(self.box_vol(self.box_int(embedding_all_target, embedding_word))-torch.minimum(self.box_vol(embedding_all_target), self.box_vol(embedding_word)))\n",
    "            \n",
    "        # idx = (-sim3).argsort()\n",
    "        \n",
    "\n",
    "        sim3 = torch.exp(model.box_vol(model.box_int(embedding_all_target, embedding_word))- model.box_vol(embedding_all_target))\n",
    "\n",
    "        idx = (-sim3).argsort()\n",
    "\n",
    "        print(idx[0:50])\n",
    "\n",
    "        for i, index in enumerate(idx):\n",
    "            print(\"Similar to : \", model.vocab.lookup_token(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1852250/1157592248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmost_similar_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"daughter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1852250/2997332562.py\u001b[0m in \u001b[0;36mmost_similar_2\u001b[0;34m(word, N)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mN\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0membedding_all_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "most_similar_2(\"daughter\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
