{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AllenNLP not available. Registrable won't work.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import vocab\n",
    "from torch import nn\n",
    "import os \n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from box_embeddings.modules.volume.volume import Volume\n",
    "from box_embeddings.modules.intersection import Intersection\n",
    "sys.path.append(\"../\")\n",
    "from utils.model import BoxModel\n",
    "direc = \"/home/fmollica/word2box_pytorch/weights/skipgram_WikiText103/epochs_2_min_count_50_batch_size_32768_embed_dim_2_lr_0.001_window_5_neg_count_5\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(f'{direc}/model_final.pt', map_location=device)\n",
    "vocab = torch.load(f'{direc}/vocab.pt')\n",
    "word2vec_model = Word2Vec.load(f'{direc}/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoxModel(\n",
       "  (embeddings_word): BoxEmbedding(42038, 4)\n",
       "  (embeddings_context): BoxEmbedding(42038, 4)\n",
       "  (box_vol): Volume()\n",
       "  (box_int): Intersection()\n",
       "  (vocab): Vocab()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_2(word, N=None):\n",
    "\n",
    "        if N is None:\n",
    "            N = len(model.vocab)\n",
    "\n",
    "        embedding_all_target = model.embeddings_word.all_boxes\n",
    "\n",
    "        index_word = (model.vocab[word])\n",
    "\n",
    "        embedding_word = embedding_all_target[index_word]\n",
    "                \n",
    "        sim3 = torch.exp(model.box_vol(model.box_int(embedding_all_target, embedding_word))- torch.maximum(model.box_vol(embedding_word),model.box_vol(embedding_all_target)))\n",
    "\n",
    "        idx = (-sim3).argsort()\n",
    "\n",
    "        for i, index in enumerate(idx[0:50]):\n",
    "            print(\"Similar to : \", str(index.item()),   model.vocab.lookup_token(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "def most_similar_2_cosine(word, N=None):\n",
    "\n",
    "        if N is None:\n",
    "            N = len(model.vocab)\n",
    "\n",
    "        embedding_all_target = model.embeddings_word.all_boxes\n",
    "\n",
    "        index_word = (model.vocab[word])\n",
    "                \n",
    "        centroid_word = ((embedding_all_target[index_word].z + embedding_all_target[index_word].Z)/2)       \n",
    "\n",
    "        centroid_others = ((embedding_all_target.z + embedding_all_target.Z)/2)\n",
    "        \n",
    "        cosine = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "        sim3 = cosine(centroid_word, centroid_others)\n",
    "\n",
    "        idx = (-sim3).argsort()\n",
    "\n",
    "        for i, index in enumerate(idx[0:50]):\n",
    "            print(\"Similar to : \", str(index.item()),   model.vocab.lookup_token(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar to :  32 first\n",
      "Similar to :  130 portrait\n",
      "Similar to :  133 voiced\n",
      "Similar to :  197 expense\n",
      "Similar to :  235 learn\n",
      "Similar to :  240 used\n",
      "Similar to :  257 target\n",
      "Similar to :  430 new\n",
      "Similar to :  451 begin\n",
      "Similar to :  461 look\n",
      "Similar to :  486 upon\n",
      "Similar to :  513 removed\n",
      "Similar to :  525 component\n",
      "Similar to :  535 needing\n",
      "Similar to :  649 bonus\n",
      "Similar to :  650 people\n",
      "Similar to :  652 transfer\n",
      "Similar to :  748 richard\n",
      "Similar to :  921 budget\n",
      "Similar to :  955 determine\n",
      "Similar to :  964 massey\n",
      "Similar to :  987 already\n",
      "Similar to :  1056 collision\n",
      "Similar to :  1074 provision\n",
      "Similar to :  1186 manufacturing\n",
      "Similar to :  1235 paper\n",
      "Similar to :  1289 shop\n",
      "Similar to :  1292 etc\n",
      "Similar to :  1311 ball\n",
      "Similar to :  1316 police\n",
      "Similar to :  1427 furnish\n",
      "Similar to :  1456 largest\n",
      "Similar to :  1474 historic\n",
      "Similar to :  1549 theory\n",
      "Similar to :  1554 biography\n",
      "Similar to :  1599 afterward\n",
      "Similar to :  1649 studio\n",
      "Similar to :  1687 often\n",
      "Similar to :  1721 planning\n",
      "Similar to :  1725 edith\n",
      "Similar to :  1742 competent\n",
      "Similar to :  1748 tried\n",
      "Similar to :  1788 jacket\n",
      "Similar to :  1862 rep\n",
      "Similar to :  1876 prayer\n",
      "Similar to :  1883 cecily\n",
      "Similar to :  2108 elongated\n",
      "Similar to :  2146 continental\n",
      "Similar to :  2156 report\n",
      "Similar to :  2191 term\n"
     ]
    }
   ],
   "source": [
    "most_similar_2_cosine(\"medicine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar to :  1204 medicine\n",
      "Similar to :  25929 mesh\n",
      "Similar to :  16207 reasonably\n",
      "Similar to :  12018 consistency\n",
      "Similar to :  5202 runway\n",
      "Similar to :  8644 episcopal\n",
      "Similar to :  20500 mitovich\n",
      "Similar to :  10011 deadly\n",
      "Similar to :  13145 retro\n",
      "Similar to :  23558 outspoken\n",
      "Similar to :  15798 prescription\n",
      "Similar to :  6096 uprising\n",
      "Similar to :  6245 downward\n",
      "Similar to :  21444 interactivity\n",
      "Similar to :  11023 sourced\n",
      "Similar to :  24583 harness\n",
      "Similar to :  27594 carefree\n",
      "Similar to :  15262 carlo\n",
      "Similar to :  18033 muncie\n",
      "Similar to :  5946 trans\n",
      "Similar to :  22079 edged\n",
      "Similar to :  2707 mercury\n",
      "Similar to :  12257 referee\n",
      "Similar to :  13283 raider\n",
      "Similar to :  20012 absorbs\n",
      "Similar to :  10336 leicestershire\n",
      "Similar to :  11506 viceroy\n",
      "Similar to :  12136 collateral\n",
      "Similar to :  10668 gut\n",
      "Similar to :  7862 twitter\n",
      "Similar to :  4967 confessed\n",
      "Similar to :  16529 slang\n",
      "Similar to :  1409 forever\n",
      "Similar to :  10791 wayne\n",
      "Similar to :  18259 menu\n",
      "Similar to :  5 battlefield\n",
      "Similar to :  3729 preferring\n",
      "Similar to :  8915 communism\n",
      "Similar to :  9120 safer\n",
      "Similar to :  23531 eloquence\n",
      "Similar to :  26938 aggravated\n",
      "Similar to :  8466 miami\n",
      "Similar to :  12334 iowa\n",
      "Similar to :  21075 diverges\n",
      "Similar to :  4227 govern\n",
      "Similar to :  20442 memling\n",
      "Similar to :  34975 mauritania\n",
      "Similar to :  6201 displeasure\n",
      "Similar to :  20103 commence\n",
      "Similar to :  7951 systematic\n"
     ]
    }
   ],
   "source": [
    "most_similar_2(\"medicine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_correlations_results(direc):\n",
    "\n",
    "    volume = model.box_vol\n",
    "    intersection = model.box_int\n",
    "\n",
    "    boxes = model.embeddings_word.all_boxes\n",
    "\n",
    "    rows_data_int = []\n",
    "    rows_data_w_int = []\n",
    "    rows_data_centroids = []\n",
    "\n",
    "    for filename in os.listdir(\"/home/fmollica/word2box_pytorch/word_similarity_dataset\"):\n",
    "        dataframe = pd.read_csv(\"/home/fmollica/word2box_pytorch/word_similarity_dataset/\" + filename)\n",
    "        dataframe.rename( columns={'Unnamed: 0':'index'}, inplace=True )\n",
    "\n",
    "        list_similarity_word2box_intersection = []\n",
    "        list_similarity_word2box_weighted_intersection = []\n",
    "        list_similarity_word2box_centroids = []\n",
    "        list_similarity_word2vec = []\n",
    "        list_similarity_sota = []\n",
    "\n",
    "        if filename!=\"men.csv\":\n",
    "            for i, row in dataframe.iterrows():\n",
    "                try:\n",
    "                    word1_word2vec = word2vec_model.wv.key_to_index[row[\"word1\"]]\n",
    "                    word2_word2vec = word2vec_model.wv.key_to_index[row[\"word2\"]]\n",
    "                    word1_word2box = vocab.lookup_indices([row[\"word1\"]])[0]\n",
    "                    word2_word2box = vocab.lookup_indices([row[\"word2\"]])[0]\n",
    "                except: \n",
    "                    continue\n",
    "                \n",
    "                list_similarity_word2vec.append(word2vec_model.wv.similarity(row[\"word1\"], row[\"word2\"]))\n",
    "                list_similarity_sota.append(row[\"similarity\"])\n",
    "                #####SIMPLE INTERSECTION BETWEEN BOXES#####\n",
    "                list_similarity_word2box_intersection.append(torch.exp(volume(intersection(boxes[word1_word2box], boxes[word2_word2box]))).item())\n",
    "                #####WEIGHTED INTERSECTION BETWEEN BOXES####\n",
    "                list_similarity_word2box_weighted_intersection.append(torch.exp(volume(intersection(boxes[word1_word2box], boxes[word2_word2box]))-torch.maximum(volume(boxes[word1_word2box]), volume(boxes[word2_word2box]))).item())\n",
    "                #####COSINE SIMILARITY BETWEEN CENTROIDS#####\n",
    "                centroid_word1 = ((boxes[word1_word2box].z + boxes[word1_word2box].Z)/2).tolist()\n",
    "                centroid_word2 = ((boxes[word2_word2box].z + boxes[word2_word2box].Z)/2).tolist()\n",
    "                list_similarity_word2box_centroids.append(1 - spatial.distance.cosine(centroid_word1, centroid_word2))\n",
    "                \n",
    "\n",
    "            rho_vec, p_vec = spearmanr(list_similarity_word2vec, list_similarity_sota)\n",
    "            rho_box_int, p_box_int = spearmanr(list_similarity_word2box_intersection, list_similarity_sota)\n",
    "            rho_box_w_int, p_box_w_int = spearmanr(list_similarity_word2box_weighted_intersection, list_similarity_sota)\n",
    "            rho_box_centroids, p_box_centroids = spearmanr(list_similarity_word2box_centroids, list_similarity_sota)\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i, row in dataframe.iterrows():\n",
    "                try:\n",
    "                    word1_word2vec = word2vec_model.wv.key_to_index[row[\"word1\"][:-2]]\n",
    "                    word2_word2vec = word2vec_model.wv.key_to_index[row[\"word2\"][:-2]]\n",
    "                    word1_word2box = vocab.lookup_indices([row[\"word1\"][:-2]])[0]\n",
    "                    word2_word2box = vocab.lookup_indices([row[\"word2\"][:-2]])[0]\n",
    "                except: \n",
    "                    continue\n",
    "                \n",
    "                list_similarity_word2vec.append(word2vec_model.wv.similarity(row[\"word1\"][:-2], row[\"word2\"][:-2]))\n",
    "                list_similarity_sota.append(row[\"similarity\"])\n",
    "                #####SIMPLE INTERSECTION BETWEEN BOXES#####\n",
    "                list_similarity_word2box_intersection.append(torch.exp(volume(intersection(boxes[word1_word2box], boxes[word2_word2box]))).item())\n",
    "                #####WEIGHTED INTERSECTION BETWEEN BOXES####\n",
    "                list_similarity_word2box_weighted_intersection.append(torch.exp(volume(intersection(boxes[word1_word2box], boxes[word2_word2box]))-torch.maximum(volume(boxes[word1_word2box]), volume(boxes[word2_word2box]))).item())\n",
    "                #####COSINE SIMILARITY BETWEEN CENTROIDS#####\n",
    "                centroid_word1 = ((boxes[word1_word2box].z + boxes[word1_word2box].Z)/2).tolist()\n",
    "                centroid_word2 = ((boxes[word2_word2box].z + boxes[word2_word2box].Z)/2).tolist()\n",
    "                list_similarity_word2box_centroids.append(1 - spatial.distance.cosine(centroid_word1, centroid_word2))\n",
    "\n",
    "            rho_vec, p_vec = spearmanr(list_similarity_word2vec, list_similarity_sota)\n",
    "            rho_box_int, p_box_int = spearmanr(list_similarity_word2box_intersection, list_similarity_sota)\n",
    "            rho_box_w_int, p_box_w_int = spearmanr(list_similarity_word2box_weighted_intersection, list_similarity_sota)\n",
    "            rho_box_centroids, p_box_centroids = spearmanr(list_similarity_word2box_centroids, list_similarity_sota)\n",
    "\n",
    "        rows_data_int.append([filename[:-4], rho_vec, p_vec, rho_box_int, p_box_int])\n",
    "        rows_data_w_int.append([filename[:-4], rho_vec, p_vec, rho_box_w_int, p_box_w_int])\n",
    "        rows_data_centroids.append([filename[:-4], rho_vec, p_vec, rho_box_centroids, p_box_centroids])\n",
    "\n",
    "    columns_ = [\"dataset\", \"rho_vec\", \"p_vec\", \"rho_box\", \"p_box\"]\n",
    "\n",
    "    dataf_int = pd.DataFrame(rows_data_int, columns = columns_)\n",
    "    dataf_w_int = pd.DataFrame(rows_data_w_int, columns = columns_)\n",
    "    dataf_centroids = pd.DataFrame(rows_data_centroids, columns = columns_)\n",
    "\n",
    "    dataf_int.to_pickle(direc + \"/simple_intersection_correlations.pkl\")\n",
    "    dataf_w_int.to_pickle(direc + \"/weighted_intersection_correlations.pkl\")\n",
    "    dataf_centroids.to_pickle(direc + \"/centroids_correlations.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_correlations_results(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>rho_vec</th>\n",
       "      <th>p_vec</th>\n",
       "      <th>rho_box</th>\n",
       "      <th>p_box</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>verb-143</td>\n",
       "      <td>0.248259</td>\n",
       "      <td>8.307688e-03</td>\n",
       "      <td>0.181384</td>\n",
       "      <td>5.562491e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mturk-771</td>\n",
       "      <td>0.525938</td>\n",
       "      <td>1.652121e-54</td>\n",
       "      <td>0.254416</td>\n",
       "      <td>1.574058e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rg-65</td>\n",
       "      <td>0.606987</td>\n",
       "      <td>8.313220e-08</td>\n",
       "      <td>0.454312</td>\n",
       "      <td>1.438440e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>men</td>\n",
       "      <td>0.566234</td>\n",
       "      <td>9.024346e-249</td>\n",
       "      <td>0.272187</td>\n",
       "      <td>4.543105e-51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wordsim353-rel</td>\n",
       "      <td>0.463134</td>\n",
       "      <td>1.106347e-13</td>\n",
       "      <td>0.146215</td>\n",
       "      <td>2.627094e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yp-130</td>\n",
       "      <td>0.225059</td>\n",
       "      <td>1.514230e-02</td>\n",
       "      <td>0.192932</td>\n",
       "      <td>3.798899e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>simverb-3500</td>\n",
       "      <td>0.120607</td>\n",
       "      <td>9.722717e-12</td>\n",
       "      <td>0.019849</td>\n",
       "      <td>2.640611e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mc-30</td>\n",
       "      <td>0.628393</td>\n",
       "      <td>2.004658e-04</td>\n",
       "      <td>0.320125</td>\n",
       "      <td>8.459837e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>simlex999</td>\n",
       "      <td>0.160929</td>\n",
       "      <td>3.826119e-07</td>\n",
       "      <td>0.051939</td>\n",
       "      <td>1.032904e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mturk-287</td>\n",
       "      <td>0.569230</td>\n",
       "      <td>4.587655e-21</td>\n",
       "      <td>0.324230</td>\n",
       "      <td>5.281393e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>semeval17</td>\n",
       "      <td>0.575549</td>\n",
       "      <td>2.021426e-27</td>\n",
       "      <td>0.272620</td>\n",
       "      <td>2.005641e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wordsim353-sim</td>\n",
       "      <td>0.613121</td>\n",
       "      <td>6.749977e-21</td>\n",
       "      <td>0.199195</td>\n",
       "      <td>5.997714e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rw</td>\n",
       "      <td>0.362997</td>\n",
       "      <td>6.039696e-19</td>\n",
       "      <td>0.152942</td>\n",
       "      <td>2.736142e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset   rho_vec          p_vec   rho_box         p_box\n",
       "0         verb-143  0.248259   8.307688e-03  0.181384  5.562491e-02\n",
       "1        mturk-771  0.525938   1.652121e-54  0.254416  1.574058e-12\n",
       "2            rg-65  0.606987   8.313220e-08  0.454312  1.438440e-04\n",
       "3              men  0.566234  9.024346e-249  0.272187  4.543105e-51\n",
       "4   wordsim353-rel  0.463134   1.106347e-13  0.146215  2.627094e-02\n",
       "5           yp-130  0.225059   1.514230e-02  0.192932  3.798899e-02\n",
       "6     simverb-3500  0.120607   9.722717e-12  0.019849  2.640611e-01\n",
       "7            mc-30  0.628393   2.004658e-04  0.320125  8.459837e-02\n",
       "8        simlex999  0.160929   3.826119e-07  0.051939  1.032904e-01\n",
       "9        mturk-287  0.569230   4.587655e-21  0.324230  5.281393e-07\n",
       "10       semeval17  0.575549   2.021426e-27  0.272620  2.005641e-06\n",
       "11  wordsim353-sim  0.613121   6.749977e-21  0.199195  5.997714e-03\n",
       "12              rw  0.362997   6.039696e-19  0.152942  2.736142e-04"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open(os.getcwd() + '/weighted_intersection_correlations.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1e2e2525b830fb771b18a2c276942b1f6f5f0685cd91ad5d2e2e18c8f29159a"
  },
  "kernelspec": {
   "display_name": "word2box_venv",
   "language": "python",
   "name": "word2box_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
